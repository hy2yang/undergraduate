utils::View(AccSub)
aggregate(AccSub,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=count)
library(class)
library(plyr) #needed for arrange
library(e1071) ## needed for Naive Bayes
#setwd("D:/BA/Homework/HW02")
aggregate(AccSub,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=count)
aggregate(AccSub$MAX_SEV_IR,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=count)
aggregate(AccSub$MAX_SEV_IR,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=sum(COUNT))
aggregate(AccSub$MAX_SEV_IR,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=sum(AccSub$COUNT))
aggregate(AccSub$COUNT,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=sum)
suba<-aggregate(AccSub$COUNT,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=sum)
subA<-aggregate(AccSub$COUNT,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=sum)
rm(suba)
AccSubAg<-aggregate(AccSub$COUNT,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=sum)
rm(subA)
?arrange
arrange(AccSubAg,WEATHER_R,TRAF_CON_R)
utils::View(AccSubAg)
AccSubAg<-aggregate(COUNT,by=list(WEATHER_R,TRAF_CON_R,MAX_SEV_IR),FUN=sum,data=AccSub)
?naiveBayes
'P(MAX_SEV_IR=1|(WEATHER_R=2，TRAF_CON_R=0))=1/(5+1)=1/6'
attach(AccSub)
AccSubAg<-aggregate(COUNT,by=list(WEATHER_R,TRAF_CON_R,MAX_SEV_IR),FUN=sum)
detach(AccSub)
arrange(AccSubAg,WEATHER_R,TRAF_CON_R)
(AccSubAg<-aggregate(COUNT,by=list(WEATHER_R,TRAF_CON_R,MAX_SEV_IR),FUN=sum))
attach(AccSub)
(AccSubAg<-aggregate(COUNT,by=list(WEATHER_R,TRAF_CON_R,MAX_SEV_IR),FUN=sum))
detach(AccSub)
arrange(AccSubAg,WEATHER_R,TRAF_CON_R)
?fix
fix(View())
fix(AccSub)
attach(AccSub)
table(MAX_SEV_IR,WEATHER_R)
table(MAX_SEV_IR,TRAF_CON_R)
table(MAX_SEV_IR)
detach(AccSub)
arrange(AccSubAg,WEATHER_R,TRAF_CON_R)
AccSubAg<-aggregate(AccSub$COUNT,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=sum)
arrange(AccSubAg,WEATHER_R,TRAF_CON_R)
AccSub[2,]
Predictors <- AccSub[,c('WEATHER_R','TRAF_CON_R')]
Target <- AccSub[,'MAX_SEV_IR']
classifier<-naiveBayes(Predictors, Target)
Probs <- predict(classifier, Predictors, type='raw',threshold=0.01)
Probs
AccSub <- data.frame(AccSub,PredictInj = rep(0:1,6))
AccSub <- Accidents[c(1:12),c('WEATHER_R','TRAF_CON_R','MAX_SEV_IR')]
AccSub <- data.frame(AccSub,COUNT=rep(1,nrow(AccSub)))
AccSub[2,]
Predictors <- AccSub[,c('WEATHER_R','TRAF_CON_R')]
Target <- AccSub[,'MAX_SEV_IR']
classifier<-naiveBayes(Predictors, Target)
Probs <- predict(classifier, Predictors, type='raw',threshold=0.01)
Probs
#在AccSub中新增加一列，列名为PredictInj，初始值为0,1,0,1...
AccSub <- data.frame(AccSub,PredictInj = rep(0:1,6))
View(AccSub)
utils::View(AccSub)
?predict
Probs <- predict(classifier, Predictors, type='raw',threshold=0.4)
Probs
Probs <- predict(classifier, Predictors, type='raw',threshold=0.01)
Probs
AccSub <- data.frame(AccSub,PredictInj = rep(0:1,6))
p_s_index <- which(Probs[,2]>0.4)
(p_s_index <- which(Probs[,2]>0.4))
(p_s_index[,which(Probs[,2]>0.4)] <- 1)
rm(p_s_index)
(Probs[,2]>0.4)
(1(Probs[,2]>0.4))
AccSub[,'PredictInj']<-1*(Probs[,2]>0.4)
AccSub[,'PredictInj']
table(AccSub[,3:4])
table(AccSub[,2:4])
table(AccSub[,3:5])
table(AccSub[,c(3,4)])
table(AccSub[,c(3,5)])
AccSub <- data.frame(AccSub,PredictInj = rep(0:1,6))
AccSub <- Accidents[c(1:12),c('WEATHER_R','TRAF_CON_R','MAX_SEV_IR')]
AccSub <- data.frame(AccSub,COUNT=rep(1,nrow(AccSub)))
AccSub <- data.frame(AccSub,PredictInj = rep(0:1,6))
(AccSub[,'PredictInj']<-1*(Probs[,2]>0.4))
table(AccSub[,c(3,5)])
AccidentModel <- Accidents[,c('HOUR_I_R','ALIGN_I','WRK_ZONE','WKDY_I_R','INT_HWY','RELJCT_I_R',
'REL_RWY_R','TRAF_CON_R','TRAF_WAY','MAX_SEV_IR')]
set.seed(1000)
RowNum <- nrow(AccidentModel)
SampleIndex <- sample(1:RowNum,round(RowNum*0.8),replace = FALSE)
TrainData <- AccidentModel[SampleIndex,]
ValidationData <- AccidentModel[-SampleIndex,]
TargetIndex <- which(colnames(AccidentModel)=='MAX_SEV_IR')
Predictors <- TrainData[,-TargetIndex]
library(class)
library(plyr) #needed for arrange
library(e1071) ## needed for Naive Bayes
classifier<-naiveBayes(Predictors,TrainData[,TargetIndex])
table(predict(classifier, Predictors), ValidationData[,TargetIndex])
?naiveBayes()
(predict(classifier, ValidationData[,-TargetIndex]), ValidationData[,TargetIndex])
?predict
MyPredict<-predict(classifier, ValidationData[,-TargetIndex])
table(MyPredict,ValidationData[,TargetIndex])
blind<-naiveBayes(TrainData[,TargetIndex])
blind<-naiveBayes(TrainData[,TargetIndex],TrainData[,TargetIndex])
blindP<- predict(blind, ValidationData[,TargetIndex])
table(blindP,ValidationData[,TargetIndex])
summary(ValidationData[,TargetIndex])
?table()
table(ValidationData[,TargetIndex],blindP)
table(blindP,ValidationData[,TargetIndex])
r1<-table(MyPredict,ValidationData[,TargetIndex])
(r1<-table(MyPredict,ValidationData[,TargetIndex]))
rm(r1)
4176/(4176+4261)
(1683+2265)/(1683+2265+1911+2578)
fgl <- read.csv("fgl.csv",header = TRUE)
library(caret)
install.packages("caret")
library("caret", lib.loc="D:/PROF/R/R-3.3.3/library")
install.packages("lazyeval")
library("caret", lib.loc="D:/PROF/R/R-3.3.3/library")
View(fgl)
Predictors <- fgl[,-'type']
Predictors <- fgl[,-10]
model <- train(Predictors, fgl$type,method='knn',
tuneGrid = data.frame(k=1:8),
metric='Accuracy',
trControl=trainControl(method='repeatedcv', number=5, repeats=20)
)
model <- train(Predictors, fgl[,10],method='knn',
tuneGrid = data.frame(k=1:8),
metric='Accuracy',
trControl=trainControl(method='repeatedcv', number=5, repeats=20)
)
plot(model)
confusionMatrix(model)
library(caret)
#setwd("D:/BA/Homework/HW02")
fgl <- read.csv("fgl.csv",header = TRUE)
Predictors <- fgl[,-10]
model <- train(Predictors, fgl[,10],method='knn',
tuneGrid = data.frame(k=1:8),
metric='Accuracy',
trControl=trainControl(method='repeatedcv', number=5, repeats=20)
)
plot(model)
confusionMatrix(model)
library(class)
library(plyr) #needed for arrange
library(e1071) ## needed for Naive Bayes
#setwd("D:/BA/Homework/HW02")
Accidents <- read.csv("Accidents.csv",header = TRUE)
#将MAX_SEV_IR等于1或者2的值全部设置为1。
Accidents[Accidents[,'MAX_SEV_IR']>0,'MAX_SEV_IR'] = 1
#将1-13,15-17,19-20,22-24列转换成类别型变量。
AA <- c(1:13,15:17,19:20,22:24);
for (i in 1:length(AA)) {
Accidents[,AA[i]] <- factor(Accidents[,AA[i]])
}
#Q1.1
#取出Accidents数据集的子集前12条记录，只取WEATHER_R和TRAF_CON_R为预测变量。
AccSub <- Accidents[c(1:12),c('WEATHER_R','TRAF_CON_R','MAX_SEV_IR')]
AccSub <- data.frame(AccSub,COUNT=rep(1,nrow(AccSub)))
#下面列出分类汇总表：
AccSubAg<-aggregate(AccSub$COUNT,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=sum)
arrange(AccSubAg,WEATHER_R,TRAF_CON_R)
#Q1.2
'P(MAX_SEV_IR=1 | (WEATHER_R=2,TRAF_CON_R=0))=1/(5+1)=1/6'
#Q1.3
attach(AccSub)
table(MAX_SEV_IR,WEATHER_R)
table(MAX_SEV_IR,TRAF_CON_R)
table(MAX_SEV_IR)
detach(AccSub)
AccSub[2,]
Predictors <- AccSub[,c('WEATHER_R','TRAF_CON_R')]
Target <- AccSub[,'MAX_SEV_IR']
classifier<-naiveBayes(Predictors, Target)
Probs <- predict(classifier, Predictors, type='raw',threshold=0.01)
Probs
#在AccSub中新增加一列，列名为PredictInj，初始值为0,1,0,1...
AccSub <- data.frame(AccSub,PredictInj = rep(0:1,6))
(AccSub[,'PredictInj']<-1*(Probs[,2]>0.4))
(AccSub[,'PredictInj']<-as.factor(1*(Probs[,2]>0.4)))
4176/(4176+4261)
(1683+2265)/(1683+2265+1911+2578)
library(class)
library(plyr) #needed for arrange
library(e1071) ## needed for Naive Bayes
#setwd("D:/BA/Homework/HW02")
?naiveBayes
?predict
?train
library(caret) ## needed for tuning Naive Bayes models
?train
?trainlibrary(combinat) ## needed for package klaR
?ifelse
library(class)
library(plyr) #needed for arrange
library(e1071) ## needed for Naive Bayes
#setwd("D:/BA/Homework/HW02")
Accidents <- read.csv("Accidents.csv",header = TRUE)
#将MAX_SEV_IR等于1或者2的值全部设置为1。
Accidents[Accidents[,'MAX_SEV_IR']>0,'MAX_SEV_IR'] = 1
#将1-13,15-17,19-20,22-24列转换成类别型变量。
AA <- c(1:13,15:17,19:20,22:24);
for (i in 1:length(AA)) {
Accidents[,AA[i]] <- factor(Accidents[,AA[i]])
}
#Q1.1
#取出Accidents数据集的子集前12条记录，只取WEATHER_R和TRAF_CON_R为预测变量。
AccSub <- Accidents[c(1:12),c('WEATHER_R','TRAF_CON_R','MAX_SEV_IR')]
AccSub <- data.frame(AccSub,COUNT=rep(1,nrow(AccSub)))
#下面列出分类汇总表：
AccSubAg<-aggregate(AccSub$COUNT,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=sum)
arrange(AccSubAg,WEATHER_R,TRAF_CON_R)
#Q1.2
'P(MAX_SEV_IR=1 | (WEATHER_R=2,TRAF_CON_R=0))=1/(5+1)=1/6'
#Q1.3
attach(AccSub)
table(MAX_SEV_IR,WEATHER_R)
table(MAX_SEV_IR,TRAF_CON_R)
table(MAX_SEV_IR)
detach(AccSub)
'P(MAX_SEV_IR=1 | (WEATHER_R=2,TRAF_CON_R=0))=(1/4 * 1 * 1/3)/((1/4 * 1 * 1/3)+(3/4 * 2/3 * 2/3))=1/5'
#Q1.4: 使用Naive Bayes模型
AccSub[2,]
Predictors <- AccSub[,c('WEATHER_R','TRAF_CON_R')]
Target <- AccSub[,'MAX_SEV_IR']
classifier<-naiveBayes(Predictors, Target)
Probs <- predict(classifier, Predictors, type='raw',threshold=0.01)
Probs
#在AccSub中新增加一列，列名为PredictInj，初始值为0,1,0,1...
AccSub <- data.frame(AccSub,PredictInj = rep(0:1,6))
(AccSub[,'PredictInj']<-1*(Probs[,2]>0.4))
table(AccSub[,c(3,5)])
#Q1.5
AccidentModel <- Accidents[,c('HOUR_I_R','ALIGN_I','WRK_ZONE','WKDY_I_R','INT_HWY','RELJCT_I_R',
'REL_RWY_R','TRAF_CON_R','TRAF_WAY','MAX_SEV_IR')]
#以HOUR_I_R,ALIGN_I...'MAX_SEV_IR'建立模型
set.seed(1000)
#固定随机数种子为1000
RowNum <- nrow(AccidentModel)
#取模型中行数
SampleIndex <- sample(1:RowNum,round(RowNum*0.8),replace = FALSE)
#前80%的行索引不放回取出作为样本
TrainData <- AccidentModel[SampleIndex,]
#样本定义为训练集
ValidationData <- AccidentModel[-SampleIndex,]
#后20%定义为验证集
TargetIndex <- which(colnames(AccidentModel)=='MAX_SEV_IR')
#预测目标为名为MAX_SEV_IR的列
Predictors <- TrainData[,-TargetIndex]
#其余为预测因子
#Q1.6
#注意下面的空白处是由你来填写代码，而不是让你通过注释回答问题。
#对问题的所有回答都应该写在Word文档里面。
#不过你可以写注释帮助TA理解你写的代码是什么意思，以便你在出错时TA可以找到理由给你部分分数。
classifier<-naiveBayes(Predictors,TrainData[,TargetIndex])
MyPredict<-predict(classifier, ValidationData[,-TargetIndex])
table(MyPredict,ValidationData[,TargetIndex])
#Q1.7
blind<-naiveBayes(TrainData[,TargetIndex],TrainData[,TargetIndex])
blindP<- predict(blind, ValidationData[,TargetIndex])
table(blindP,ValidationData[,TargetIndex])
#全预测为1
#Q1.8
4176/(4176+4261)
(1683+2265)/(1683+2265+1911+2578)
#朴素贝叶斯分类器错误率略小
#Q1.9
classifier
model <- train(
Predictors,
TrainData[,TargetIndex],
#先列出Predictors,然后是结果变量，此处是Flight.Status。
method='nb',
# method = 'nb' 指的是使用Naive Bayes
metric='Accuracy',
#评价指标是“准确率” Accuracy
trControl=trainControl(
method='repeatedcv',  number=10,  repeats=2) )
model
confusionMatrix(model)
model$finalModel
MyPredict$posterior
data <- data.frame(predict(classifier,Predictors)$posterior)
(predict(classifier,Predictors)$posterior)
(predict(classifier,Predictors))
?pred
data <- data.frame(predict(model$finalModel,Predictors)$posterior)
View(data)
utils::View(data)
rm(data)
data <- data.frame(predict(classifier,Predictors)$posterior)
data <- data.frame(predict(classifier,Predictors)[,'posterior'])
utils::View(model$finalModel)
utils::View(data.frame(model$finalModel))
summaty(model$finalModel)
summary(model$finalModel)
predict(classifier,Predictors)$posterior
summary(predict(classifier,Predictors))
library(caret)
library("caret", lib.loc="D:/PROF/R/R-3.3.3/library")
library("klaR", lib.loc="D:/PROF/R/R-3.3.3/library")
library("MASS", lib.loc="D:/PROF/R/R-3.3.3/library")
library("class", lib.loc="D:/PROF/R/R-3.3.3/library")
library("e1071", lib.loc="D:/PROF/R/R-3.3.3/library")
library("plyr", lib.loc="D:/PROF/R/R-3.3.3/library")
data <- data.frame(predict(classifier,Predictors)$posterior)
summary(predict(classifier,Predictors))
summary(predict(model$finalModel,Predictors))
?train
model <- train(
Predictors,
TrainData[,TargetIndex],
#先列出Predictors,然后是结果变量，此处是Flight.Status。
method='nb',
# method = 'nb' 指的是使用Naive Bayes
metric='Accuracy',
#评价指标是“准确率” Accuracy
trControl=trainControl(
method='none') )
model <- train(
Predictors,
TrainData[,TargetIndex],
#先列出Predictors,然后是结果变量，此处是Flight.Status。
method='nb',
# method = 'nb' 指的是使用Naive Bayes
metric='Accuracy',
#评价指标是“准确率” Accuracy
trControl = trainControl(), tuneGrid = NULL, tuneLength = 1 )
?predict
?train
head(data)
?predict.naiveBayes
table(blindP,ValidationData[,TargetIndex])
rm(model)
(predict(classifier, ValidationData[,-TargetIndex],type ='raw'))
rawP<-(predict(classifier, ValidationData[,-TargetIndex],type ='raw'))
View(Probs)
utils::View(rawP)
rm(rawP)
rawP<-data.frame((predict(classifier, ValidationData[,-TargetIndex],type ='raw')))
rawP$p<-rawP[,2]/(rawP[,1]+rawP[,2])
rawP$p<-NULL
myTable <- data.frame(cutoff=NULL,Loss=NULL)
min(rawP$X1)
max(rawP$X1)
?round
round(min(rawP$X1),digits = 2)
round(max(rawP$X1),digits = 2)
for (i in seq(round(min(rawP$X1),digits = 2),round(max(rawP$X1),digits = 2),0.01)) {
mypred <- data.frame(pred =ifelse(rawP[,'X1']>i,'1','0'))
#如果判断为delayed的概率大于截值则认为该记录的航班会delayed。
myLoss <- sum(mypred[,'pred']!=ValidationData[,TargetIndex] & ValidationData[,TargetIndex]=='0')*1.2+
sum(mypred[,'pred']!=ValidationData[,TargetIndex] & ValidationData[,TargetIndex]=='ontime')
#如果把delayed错判为ontime，成本是3。但是如果把ontime错判为delayed，成本只有1。
myTable <- rbind(myTable,data.frame(cutoff=i,Loss=myLoss))
#把每步尝试的截值以及该截值所对应的损失合并起来组成一个数据框。
}
for (i in seq(round(min(rawP$X1),digits = 2),round(max(rawP$X1),digits = 2),0.01)) {
mypred <- data.frame(pred =ifelse(rawP[,'X1']>i,'1','0'))
#myLoss <- sum(mypred[,'pred']!=ValidationData[,TargetIndex] & ValidationData[,TargetIndex]=='0')*1.2+
#  sum(mypred[,'pred']!=ValidationData[,TargetIndex] & ValidationData[,TargetIndex]=='ontime')
# myTable <- rbind(myTable,data.frame(cutoff=i,Loss=myLoss))
}
utils::View(mypred)
sum(mypred[,'pred']!=ValidationData[,TargetIndex] & ValidationData[,TargetIndex]=='0')*1.2+
sum(mypred[,'pred']!=ValidationData[,TargetIndex] & ValidationData[,TargetIndex]=='ontime')
?ifelse
for (i in seq(round(min(rawP$X1),digits = 2),round(max(rawP$X1),digits = 2),0.01)) {
mypred <- data.frame(pred =ifelse(rawP[,'X1']>i,1,0))
#myLoss <- sum(mypred[,'pred']!=ValidationData[,TargetIndex] & ValidationData[,TargetIndex]=='0')*1.2+
#  sum(mypred[,'pred']!=ValidationData[,TargetIndex] & ValidationData[,TargetIndex]=='ontime')
# myTable <- rbind(myTable,data.frame(cutoff=i,Loss=myLoss))
}
sum(mypred[,'pred']!=ValidationData[,'MAX_SEV_IR'] & ValidationData[,'MAX_SEV_IR']=='0')*1.2+
sum(mypred[,'pred']!=ValidationData[,'MAX_SEV_IR'] & ValidationData[,'MAX_SEV_IR']=='1')
for (i in seq(round(min(rawP$X1),digits = 2),round(max(rawP$X1),digits = 2),0.01)) {
mypred <- data.frame(pred =ifelse(rawP[,'X1']>i,1,0))
myLoss <- sum(mypred[,'pred']!=ValidationData[,'MAX_SEV_IR'] & ValidationData[,'MAX_SEV_IR']=='0')*1.2+
sum(mypred[,'pred']!=ValidationData[,'MAX_SEV_IR'] & ValidationData[,'MAX_SEV_IR']=='1')
myTable <- rbind(myTable,data.frame(cutoff=i,Loss=myLoss))
}
View(myTable)
utils::View(myTable)
myTable <- data.frame(cutoff=NULL,Loss=NULL)
for (i in seq(round(min(rawP$X1),digits = 2),round(max(rawP$X1),digits = 2),0.001)) {
mypred <- data.frame(pred =ifelse(rawP[,'X1']>i,1,0))
myLoss <- sum(mypred[,'pred']!=ValidationData[,'MAX_SEV_IR'] & ValidationData[,'MAX_SEV_IR']=='0')*1.2+
sum(mypred[,'pred']!=ValidationData[,'MAX_SEV_IR'] & ValidationData[,'MAX_SEV_IR']=='1')
myTable <- rbind(myTable,data.frame(cutoff=i,Loss=myLoss))
}
View(myTable)
utils::View(myTable)
plot(myTable)
library("ggplot2", lib.loc="D:/PROF/R/R-3.3.3/library")
library(ggplot2)
?qplot
for (i in seq(round(min(rawP$X1),digits = 2),round(max(rawP$X1),digits = 2),0.001)) {
mypred <- data.frame(pred =ifelse(rawP[,'X1']>i,'1','0'))
myLoss <- sum(mypred[,'pred']!=ValidationData[,'MAX_SEV_IR'] & ValidationData[,'MAX_SEV_IR']=='0')*1.2+
sum(mypred[,'pred']!=ValidationData[,'MAX_SEV_IR'] & ValidationData[,'MAX_SEV_IR']=='1')
myTable <- rbind(myTable,data.frame(cutoff=i,Loss=myLoss))
}
for (i in seq(round(min(rawP$X1),digits = 2),round(max(rawP$X1),digits = 2),0.001)) {
mypred <- data.frame(pred =ifelse(rawP[,'X1']>i,1,0))
myLoss <- sum(mypred[,'pred']!=ValidationData[,'MAX_SEV_IR'] & ValidationData[,'MAX_SEV_IR']=='0')*1.2+
sum(mypred[,'pred']!=ValidationData[,'MAX_SEV_IR'] & ValidationData[,'MAX_SEV_IR']=='1')
myTable <- rbind(myTable,data.frame(cutoff=i,Loss=myLoss))
}
qplot(cutoff,Loss,data=myTable,geom ="line")
idx <- which.min(myTable[,"Loss"])
myTable[idx,]
optcutoff <- myTable[idx,"cutoff"]
mypred <- data.frame(pred =ifelse(rawP[,'X1']>optcutoff,1,0),
obs = ValidationData[,'MAX_SEV_IR'])
utils::View(mypred)
table(mypred)
(911+3127)/(911+3127+3265+1134)
utils::View(myTable)
library(class)
library(plyr) #needed for arrange
library(e1071) ## needed for Naive Bayes
library("class", lib.loc="D:/PROF/R/R-3.3.3/library")
library("plyr", lib.loc="D:/PROF/R/R-3.3.3/library")
library("e1071", lib.loc="D:/PROF/R/R-3.3.3/library")
Accidents <- read.csv("Accidents.csv",header = TRUE)
Accidents[Accidents[,'MAX_SEV_IR']>0,'MAX_SEV_IR'] = 1
AA <- c(1:13,15:17,19:20,22:24);
for (i in 1:length(AA)) {
Accidents[,AA[i]] <- factor(Accidents[,AA[i]])
}
AccSub <- Accidents[c(1:12),c('WEATHER_R','TRAF_CON_R','MAX_SEV_IR')]
AccSub <- data.frame(AccSub,COUNT=rep(1,nrow(AccSub)))
AccSubAg<-aggregate(AccSub$COUNT,by=list(WEATHER_R=AccSub[,1],TRAF_CON_R=AccSub[,2],MAX_SEV_IR=AccSub[,3]),FUN=sum)
arrange(AccSubAg,WEATHER_R,TRAF_CON_R)
'P(MAX_SEV_IR=1 | (WEATHER_R=2,TRAF_CON_R=0))=1/(5+1)=1/6'
attach(AccSub)
table(MAX_SEV_IR,WEATHER_R)
table(MAX_SEV_IR,TRAF_CON_R)
table(MAX_SEV_IR)
detach(AccSub)
'P(MAX_SEV_IR=1 | (WEATHER_R=2,TRAF_CON_R=0))=(1/4 * 1 * 1/3)/((1/4 * 1 * 1/3)+(3/4 * 2/3 * 2/3))=1/5'
AccSub[2,]
Predictors <- AccSub[,c('WEATHER_R','TRAF_CON_R')]
Target <- AccSub[,'MAX_SEV_IR']
classifier<-naiveBayes(Predictors, Target)
Probs <- predict(classifier, Predictors, type='raw',threshold=0.01)
Probs
AccSub <- data.frame(AccSub,PredictInj = rep(0:1,6))
(AccSub[,'PredictInj']<-1*(Probs[,2]>0.4))
table(AccSub[,c(3,5)])
AccidentModel <- Accidents[,c('HOUR_I_R','ALIGN_I','WRK_ZONE','WKDY_I_R','INT_HWY','RELJCT_I_R',
'REL_RWY_R','TRAF_CON_R','TRAF_WAY','MAX_SEV_IR')]
#以HOUR_I_R,ALIGN_I...'MAX_SEV_IR'建立模型
set.seed(1000)
#固定随机数种子为1000
RowNum <- nrow(AccidentModel)
#取模型中行数
SampleIndex <- sample(1:RowNum,round(RowNum*0.8),replace = FALSE)
#前80%的行索引不放回取出作为样本
TrainData <- AccidentModel[SampleIndex,]
#样本定义为训练集
ValidationData <- AccidentModel[-SampleIndex,]
#后20%定义为验证集
TargetIndex <- which(colnames(AccidentModel)=='MAX_SEV_IR')
#预测目标为名为MAX_SEV_IR的列
Predictors <- TrainData[,-TargetIndex]
#其余为预测因子
classifier<-naiveBayes(Predictors,TrainData[,TargetIndex])
MyPredict<-predict(classifier, ValidationData[,-TargetIndex])
table(MyPredict,ValidationData[,TargetIndex])
blind<-naiveBayes(TrainData[,TargetIndex],TrainData[,TargetIndex])
blindP<- predict(blind, ValidationData[,TargetIndex])
table(blindP,ValidationData[,TargetIndex])
4176/(4176+4261)
(1683+2265)/(1683+2265+1911+2578)
rawP<-data.frame((predict(classifier, ValidationData[,-TargetIndex],type ='raw')))
myTable <- data.frame(cutoff=NULL,Loss=NULL)
for (i in seq(round(min(rawP$X1),digits = 2),round(max(rawP$X1),digits = 2),0.001)) {
mypred <- data.frame(pred =ifelse(rawP[,'X1']>i,1,0))
myLoss <- sum(mypred[,'pred']!=ValidationData[,'MAX_SEV_IR'] & ValidationData[,'MAX_SEV_IR']=='0')*1.2+
sum(mypred[,'pred']!=ValidationData[,'MAX_SEV_IR'] & ValidationData[,'MAX_SEV_IR']=='1')
myTable <- rbind(myTable,data.frame(cutoff=i,Loss=myLoss))
}
View(myTable)
library(ggplot2)
qplot(cutoff,Loss,data=myTable,geom ="line")
idx <- which.min(myTable[,"Loss"])
myTable[idx,]
optcutoff <- myTable[idx,"cutoff"]
mypred <- data.frame(pred =ifelse(rawP[,'X1']>optcutoff,1,0),
obs = ValidationData[,'MAX_SEV_IR'])
(optcutoff <- myTable[idx,"cutoff"])
mypred <- data.frame(pred =ifelse(rawP[,'X1']>optcutoff,1,0),
obs = ValidationData[,'MAX_SEV_IR'])
table(mypred)
library(caret)
fgl <- read.csv("fgl.csv",header = TRUE)
Predictors <- fgl[,-10]
model <- train(Predictors, fgl[,10],method='knn',
tuneGrid = data.frame(k=1:8),
metric='Accuracy',
trControl=trainControl(method='repeatedcv', number=5, repeats=20)
)
plot(model)
confusionMatrix(model)
